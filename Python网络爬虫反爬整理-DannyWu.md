# Python网络爬虫反爬策略整理 | DannyWu

## 一、常见的反爬策略
#### 1. 有些网站会通过用户代理对爬虫进行限制，只要不是浏览器访问或者一直都是某个浏览器访问，那么就限制该用户不能对网站进行访问；遇到这种情况，我们一般会采用用户代理池的方式进行解决。
#### 2. 还有些网站会通过用户访问站点时的ip进行限制，比如某一个ip在短时间内大量的访问该网站上的网页，则封掉该ip，封掉之后使用该IP就无法访问该站点了；如果遇到这种情况，我们一般会通过IP代理池的方式进行解决。
#### 3. 除此之外，有的网站会通过验证码对用户的访问请求进行限制，比如当一个用户多次访问该站点之后，会出现验证码，输入验证码之后才可以继续访问，而怎么样让爬虫自动的识别验证码是一个关键问题；如果遇到验证码从而阻挡了爬虫的运行，我们可以使用验证码自动识别的方式去处理验证码。
#### 4. 还有的网站会通过数据屏蔽的方式进行反爬，比如用户访问时出现的数据并不会出现在源码中，此时这些数据会隐藏在js文件中，以此避免爬虫对这些数据的抓取。如果遇到这种情况，我们一般会采用抓包分析去找到被屏蔽的数据，并自动获取。
